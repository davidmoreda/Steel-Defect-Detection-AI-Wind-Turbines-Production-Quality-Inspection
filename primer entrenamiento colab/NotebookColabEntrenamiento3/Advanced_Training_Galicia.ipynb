{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udfed Advanced Segmentation Training (Galicia Toshiba Dataset)\n",
    "**Objective**: Train a U-Net++ model to detect 4 classes of industrial defects on large resolution images.\n",
    "\n",
    "**Key Features**:\n",
    "1.  **Smart Sampling**: Instead of random crops, we pre-scan the dataset to find where the defects are.\n",
    "2.  **Class Oversampling**: Rare classes (e.g. Class 2) are sampled much more frequently.\n",
    "3.  **Weighted Loss**: The loss function penalizes mistakes on rare classes much more heavily.\n",
    "4.  **Full Image Inference**: Visualization block to sliding-window predict entire original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SETUP & IMPORTS\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# Check GPU\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\ud83d\ude80 Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. CONFIGURATION\n",
    "# ======================\n",
    "BASE_DIR = \"/home/dmore/code/TFM_David/CNN_Galicia_Toshiba/datasets/unet_dataset\"\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(BASE_DIR, \"val\")\n",
    "\n",
    "# Classes: 0=Safe, 1=DefectA, 2=DefectB, 3=DefectC\n",
    "CLASSES = 4 \n",
    "TILE_SIZE = 1024\n",
    "BATCH_SIZE = 4 \n",
    "\n",
    "# Training Hyperparams\n",
    "EPOCHS = 30\n",
    "PATCHES_PER_IMAGE = 20 # Effectively increases 'epoch' size to cover more area\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Penalization Weights (Higher = More focus on that class)\n",
    "# Rough estimate based on frequency: Class 2 is 10x rarer than Class 1/3\n",
    "CLASS_WEIGHTS = [0.1, 5.0, 50.0, 5.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. SMART PRE-SCANNING\n",
    "# ======================\n",
    "def scan_dataset_defects(root_dir):\n",
    "    \"\"\"\n",
    "    Scans all mask files to find coordinates of defects.\n",
    "    Returns a dict: { class_id: [ (filename, center_y, center_x), ... ] }\n",
    "    \"\"\"\n",
    "    print(f\"\ud83d\udd75\ufe0f\u200d\u2642\ufe0f Scanning {root_dir} for defects (this happens once)...\")\n",
    "    mask_dir = os.path.join(root_dir, 'masks')\n",
    "    defect_registry = defaultdict(list)\n",
    "    \n",
    "    files = [f for f in os.listdir(mask_dir) if f.endswith(('.png', '.jpg'))]\n",
    "    \n",
    "    for f in tqdm(files):\n",
    "        path = os.path.join(mask_dir, f)\n",
    "        # Load mask (0, 1, 2, 3)\n",
    "        try:\n",
    "            mask = np.array(Image.open(path))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        unique_classes = np.unique(mask)\n",
    "        \n",
    "        # Determine sampling points for each class found\n",
    "        for cls in unique_classes:\n",
    "            if cls == 0: continue # Skip background\n",
    "            \n",
    "            # Find coordinates of this defect\n",
    "            ys, xs = np.where(mask == cls)\n",
    "            \n",
    "            if len(ys) > 0:\n",
    "                # We take a few sample points from this defect blob\n",
    "                # (e.g. center, and maybe random points if it's huge)\n",
    "                # For simplicity, let's take the centroid of the blob\n",
    "                cy = int(np.mean(ys))\n",
    "                cx = int(np.mean(xs))\n",
    "                defect_registry[cls].append((f, cy, cx))\n",
    "                \n",
    "    print(\"\u2705 Scan complete.\")\n",
    "    for c, items in defect_registry.items():\n",
    "        print(f\"  - Class {c}: Found {len(items)} instances.\")\n",
    "        \n",
    "    return defect_registry\n",
    "\n",
    "# Run scan on TRAIN only\n",
    "train_defects = scan_dataset_defects(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. DATASET CLASS WITH SMART SAMPLING\n",
    "# ======================\n",
    "class IndustrialSmartDataset(Dataset):\n",
    "    def __init__(self, root_dir, defect_registry=None, transform=None, patches_per_img=10):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.img_dir = os.path.join(root_dir, 'images')\n",
    "        self.mask_dir = os.path.join(root_dir, 'masks')\n",
    "        self.patches_per_img = patches_per_img\n",
    "        \n",
    "        self.images = [f for f in os.listdir(self.img_dir) if f.lower().endswith(('.jpg', '.png'))]\n",
    "        self.defect_registry = defect_registry\n",
    "        \n",
    "        # Flatten available defects for easy random access\n",
    "        self.all_defects = []\n",
    "        if self.defect_registry:\n",
    "            for cls, items in self.defect_registry.items():\n",
    "                # Oversample rare Class 2 explicitly here if needed, \n",
    "                # but we handle it via probability below.\n",
    "                self.all_defects.extend([(cls, item) for item in items])\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Virtual length: We want to extract N patches per image per epoch\n",
    "        return len(self.images) * self.patches_per_img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Map virtual index to actual image\n",
    "        img_idx = idx % len(self.images)\n",
    "        img_name = self.images[img_idx]\n",
    "        \n",
    "        # DECISION: Defect Crop or Random Background Crop?\n",
    "        # 60% chance to force-jump to a known defect (if any exist globally)\n",
    "        use_defect_crop = (random.random() < 0.6) and (len(self.all_defects) > 0)\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        mask_name = os.path.splitext(img_name)[0] + \".png\" # Assuming png masks\n",
    "        # Fallback if mask has different ext\n",
    "        if not os.path.exists(os.path.join(self.mask_dir, mask_name)):\n",
    "             mask_name = os.path.splitext(img_name)[0] + \".jpg\"\n",
    "             \n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "        \n",
    "        # Load Full Image/Mask (Optimization: Could use partial loading libraries)\n",
    "        # But cv2 is fast enough for these sizes usually given RAM\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path, 0) # Grayscale\n",
    "        \n",
    "        h, w = mask.shape\n",
    "        crop_h, crop_w = TILE_SIZE, TILE_SIZE\n",
    "        \n",
    "        if use_defect_crop:\n",
    "            # Pick a RANDOM defect from our registry (Cross-Image Mixing)\n",
    "            # WAIT: We must crop from the CURRENT image if we loaded it.\n",
    "            # actually better strategy: \n",
    "            # 1. Pick a random defect from registry first.\n",
    "            # 2. Load THAT image. \n",
    "            # But standard Pytorch logic maps idx -> image.\n",
    "            # Let's stick to: \"If this image has defects, try to center on them\"\n",
    "            \n",
    "            # Find defects in THIS image\n",
    "            ys, xs = np.where(mask > 0)\n",
    "            if len(ys) > 0:\n",
    "                # Prioritize Class 2 if present in this image\n",
    "                ys_c2, xs_c2 = np.where(mask == 2)\n",
    "                if len(ys_c2) > 0:\n",
    "                     # High priority to Class 2\n",
    "                     center_idx = random.randint(0, len(ys_c2)-1)\n",
    "                     cy, cx = ys_c2[center_idx], xs_c2[center_idx]\n",
    "                else:\n",
    "                     center_idx = random.randint(0, len(ys)-1)\n",
    "                     cy, cx = ys[center_idx], xs[center_idx]\n",
    "                \n",
    "                # Calculate coordinates\n",
    "                y1 = max(0, min(cy - crop_h // 2, h - crop_h))\n",
    "                x1 = max(0, min(cx - crop_w // 2, w - crop_w))\n",
    "            else:\n",
    "                # Pass through to random crop\n",
    "                y1 = random.randint(0, h - crop_h)\n",
    "                x1 = random.randint(0, w - crop_w)\n",
    "        else:\n",
    "            # Completely random crop (Background sample)\n",
    "            y1 = random.randint(0, h - crop_h)\n",
    "            x1 = random.randint(0, w - crop_w)\n",
    "            \n",
    "        # Ensure bounds (in case image < tile size, though unlikely here)\n",
    "        if h < crop_h: y1 = 0\n",
    "        if w < crop_w: x1 = 0\n",
    "        \n",
    "        image = image[y1:y1+crop_h, x1:x1+crop_w]\n",
    "        mask = mask[y1:y1+crop_h, x1:x1+crop_w]\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "            \n",
    "        # Ensure mask is long (int64) for torch loss\n",
    "        return image, mask.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. AUGMENTATION PIPELINE\n",
    "# ======================\n",
    "train_transform = A.Compose([\n",
    "    A.PadIfNeeded(min_height=TILE_SIZE, min_width=TILE_SIZE, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0),\n",
    "    A.RandomCrop(height=TILE_SIZE, width=TILE_SIZE, p=1.0),\n",
    "    \n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    \n",
    "    # Brightness/Contrast to simulate different lighting\n",
    "    A.MultiplicativeNoise(multiplier=(0.9, 1.1), p=0.2),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05, p=0.3),\n",
    "    \n",
    "    # Normalize is vital for pre-trained encoders\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "], is_check_shapes=False) # Disable shape check warning\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.PadIfNeeded(min_height=TILE_SIZE, min_width=TILE_SIZE, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0),\n",
    "    A.CenterCrop(height=TILE_SIZE, width=TILE_SIZE, p=1.0),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "], is_check_shapes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. MODEL SETUP & TRAINING LOOP\n",
    "# ======================\n",
    "\n",
    "# Datasets\n",
    "train_dataset = IndustrialSmartDataset(TRAIN_DIR, defect_registry=train_defects, transform=train_transform, patches_per_img=PATCHES_PER_IMAGE)\n",
    "val_dataset = IndustrialSmartDataset(VAL_DIR, defect_registry=None, transform=val_transform, patches_per_img=5) # Scan less val\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Dataset Sizes (Virtual): Train={len(train_dataset)}, Val={len(val_dataset)}\")\n",
    "\n",
    "# Model: U-Net++ with ResNet34 Encoder\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=CLASSES,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Loss Function (Weighted)\n",
    "class_weights_t = torch.tensor(CLASS_WEIGHTS).float().to(DEVICE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_t)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, verbose=True)\n",
    "\n",
    "# Training Loop\n",
    "best_loss = float('inf')\n",
    "train_logs = []\n",
    "val_logs = []\n",
    "\n",
    "print(\"\ud83d\udd25 STARTING TRAINING...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Progress bar\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for images, masks in loop:\n",
    "        images = images.to(DEVICE)\n",
    "        masks = masks.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images) # Output: [B, 4, H, W]\n",
    "        \n",
    "        loss = loss_fn(logits, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "            logits = model(images)\n",
    "            loss = loss_fn(logits, masks)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    train_logs.append(avg_train_loss)\n",
    "    val_logs.append(avg_val_loss)\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    print(f\"   \ud83d\udcc9 Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"best_model_smart_galicia.pth\")\n",
    "        print(\"   \ud83d\udcbe Model Saved!\")\n",
    "        \n",
    "# Plot History\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_logs, label='Train')\n",
    "plt.plot(val_logs, label='Validation')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. INFERENCE & VISUALIZATION BLOCK\n",
    "# ======================\n",
    "def predict_full_image(model, img_path, tile_size=1024):\n",
    "    # Load and Normalize\n",
    "    original_img = cv2.imread(img_path)\n",
    "    original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "    h, w, c = original_img.shape\n",
    "    \n",
    "    # Pad to multiple of tile_size to avid borders issues\n",
    "    pad_h = (tile_size - (h % tile_size)) % tile_size\n",
    "    pad_w = (tile_size - (w % tile_size)) % tile_size\n",
    "    \n",
    "    padded_img = cv2.copyMakeBorder(original_img, 0, pad_h, 0, pad_w, cv2.BORDER_CONSTANT, value=0)\n",
    "    final_mask = np.zeros((padded_img.shape[0], padded_img.shape[1]), dtype=np.uint8)\n",
    "    \n",
    "    # Sliding Window\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for y in range(0, padded_img.shape[0], tile_size):\n",
    "            for x in range(0, padded_img.shape[1], tile_size):\n",
    "                tile = padded_img[y:y+tile_size, x:x+tile_size]\n",
    "                \n",
    "                # Transform\n",
    "                # Manual normalize to match Albumentations\n",
    "                tile_tensor = A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))(image=tile)['image']\n",
    "                tile_tensor = ToTensorV2()(image=tile_tensor)['image']\n",
    "                tile_tensor = tile_tensor.unsqueeze(0).to(DEVICE)\n",
    "                \n",
    "                logits = model(tile_tensor)\n",
    "                preds = torch.argmax(logits, dim=1).squeeze().cpu().numpy()\n",
    "                \n",
    "                final_mask[y:y+tile_size, x:x+tile_size] = preds\n",
    "                \n",
    "    # Crop back\n",
    "    final_mask = final_mask[:h, :w]\n",
    "    return original_img, final_mask\n",
    "\n",
    "# --- TEST ON A FEW IMAGES ---\n",
    "# Pick samples from different classes according to our scan results\n",
    "test_samples = []\n",
    "# Try to find one of each class from train data for sanity check\n",
    "for cls in [1, 2, 3]:\n",
    "    if cls in train_defects and len(train_defects[cls]) > 0:\n",
    "        fname = train_defects[cls][0][0]\n",
    "        test_samples.append(fname)\n",
    "\n",
    "# If list empty, just take randoms\n",
    "if not test_samples:\n",
    "    test_samples = os.listdir(TRAIN_DIR + '/images')[:3]\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_model_smart_galicia.pth\"))\n",
    "\n",
    "for sample_name in set(test_samples):\n",
    "    print(f\"\ud83d\udd0d Predicting {sample_name}...\")\n",
    "    full_path = os.path.join(TRAIN_DIR, 'images', sample_name)\n",
    "    if not os.path.exists(full_path): continue\n",
    "        \n",
    "    img, pred_mask = predict_full_image(model, full_path)\n",
    "    \n",
    "    # Load Ground Truth for comparison\n",
    "    gt_path = os.path.join(TRAIN_DIR, 'masks', os.path.splitext(sample_name)[0]+'.png')\n",
    "    gt_mask = cv2.imread(gt_path, 0)\n",
    "    \n",
    "    # VISUALIZE\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(gt_mask, cmap='viridis', vmin=0, vmax=3)\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(pred_mask, cmap='viridis', vmin=0, vmax=3)\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 4)\n",
    "    # Overlay Prediction on Image\n",
    "    # Create RGB mask: 0=Black, 1=Red, 2=Green, 3=Blue\n",
    "    rgb_mask = np.zeros_like(img)\n",
    "    rgb_mask[pred_mask == 1] = [255, 0, 0]   # Red\n",
    "    rgb_mask[pred_mask == 2] = [0, 255, 0]   # Green\n",
    "    rgb_mask[pred_mask == 3] = [0, 0, 255]   # Blue\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.imshow(rgb_mask, alpha=0.6)\n",
    "    plt.title(\"Overlay (Pred)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}